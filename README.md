# Crawler
Atividade de Sistema Distribuidos

A atividade baseia se em criar um Crawler

Definição :

Um crawler, que também é conhecido como web spider ou web robot, é um agente de software utilizado para automatizar pesquisa, captura e extração de dados. Trafegados geralmente em protocolos HTTP junto com o modelo TCP/IP, a maioria dos robots desenvolvidos são voltados para a web tradicional onde documentos html conectados por hyperlinks, tags e palavras chave são usados para rastrear e atender necessidades específicas como: download de imagens e arquivos, validação automática de códigos, mineração de endereços de e-mail, coleta de conteúdos específicos para cotação de preços de um determinado produto ou serviço e também pode ser desenvolvido para agregar dados de diversas fontes da internet podendo persistir essas informações em um banco de dados.
